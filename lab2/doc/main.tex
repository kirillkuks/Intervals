\documentclass[a4paper,12pt]{article}

\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{shorttoc}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{{./img/}}

\definecolor{linkcolor}{HTML}{000000}
\definecolor{urlcolor}{HTML}{0085FF}
\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\renewcommand*\contentsname{Содержание}

\newcommand{\plot}[3]{
    \begin{figure}[H]
        \begin{center}
            \includegraphics[scale=0.6]{#1}
            \caption{#2}
            \label{#3}
        \end{center}
    \end{figure}
}

\begin{document}
    \include{title}
    \newpage

    \tableofcontents
    \listoffigures
    \newpage

    \section{Постановка задачи}
    \section{Теория}
    \subsection{Точечная линейная регрессия}
    \quad Рассматривается задача восстановления зависимости для выборки
    $ (X, \textbf(Y))$, $ X = \{x_i\}_{i=1}^{n}, \textbf{Y} = \{\textbf{y}_i\}_{i=1}^{n} $,
    $ x_i $ - точеный, $ \textbf{y}_i $ - интервальный.
    Пусть искомая модель задана в классе линейных функций

    \begin{equation}
        y = \beta_0 + \beta_1 x
        \label{e:model}
    \end{equation}

    Поставим задачу оптимизацию \ref{e:task} для нахождения точечных оценок
    параметров $ \beta_0, \beta_1 $.

    \begin{equation}
        \begin{gathered}
            \sum_{i = 1}^{m}w_{i} \to \min \\
            \text{mid}\textbf{y}_{i} - w_{i} \cdot \text{rad}\textbf{y}_{i} \leq X\beta \leq \text{mid}\textbf{y}_{i} + w_{i} \cdot \text{rad}\textbf{y}_{i} \\
            w_{i} \geq 0, i = 1, ..., m \\
            w, \beta - ?
        \end{gathered}
        \label{e:task}
    \end{equation}
    
    Задачу \ref{e:task} можно решить методами линейного программирования.

    \subsection{Информационное множество}
    \quad \textsl{Информационным множеством} задачи восстановления зависимости
    будем называть множество значений всех параметров зависимости,
    совместных с данными в каком-то смысле. 

    \textsl{Коридором совместных зависимостей} задачи восстановления зависимости
    называется многозначное множество отображений $ \Upsilon $, сопоставляющее
    каждому значению аргумента $ x $ множество
    
    \begin{equation}
        \Upsilon(x) = \bigcup_{\beta \in \Omega} f(x, \beta)
    \end{equation}
    , где $ \Omega $ - информационное множество, $ x $ - вектор переменных, $ \beta $ - вектор оцениваемых параметров. 

    Информационное множество может быть построено, как пересечение полос, заданных
    
    \begin{equation}
        \underline{\textbf{y}_i} \leq \beta_0 + \beta_1 x_{i1} + ... + \beta_m x_{im} \leq \overline{\textbf{y}_i}
    \end{equation}
    , где $ i = \overline{1, n} \textbf{y}_i \in \textbf{Y}, x_i \in X $, $ X $ - точечная выборка переменных,
    $ \textbf{Y} $ - интервальная выборка откликов.

    \section{Реализация}
    \quad Весь код написан на языке Python (версии 3.7.3).
    \href{https://github.com/kirillkuks/Intervals/tree/master/lab2}{Ссылка на GitHub с исходным кодом}.

    \section{Результаты}
    \quad Данные были взяты из файлов \textsl{data/dataset1/+0\_5V/+0\_5V\_0.txt} и \textsl{data/dataset/-0\_5V/-0\_5V\_42.txt}.
    Обынтерваливание было произведено следующим образом.
    \begin{equation}
        \textbf{x}_i = [(x_i - \delta_i) - \varepsilon, (x_i - \delta_i) + \varepsilon], \varepsilon = \frac{100}{2^{14}}
    \end{equation}
    где $ x_i $ - точечное значение, $ \delta_i $ - точечная погрешность.
    Набор $ \delta_i $ получен из соответствующих файлов в \textsl{data/dataset1/ZeroLine.txt}

    Построим линейную регрессию и найдём информационное множество для двух выборок
    с разной степенью совместности.

    Рассмотрим первую выборку $ X_1 $.
    \plot{X1}{Первая выборка, $ X_1 $}{p:sampleX1}

    Индекс Жаккара первой выборки равен $ JK(X_1) = 0.5115 $ (в этой работе $ JK(X) \in [0, 1] $).

    Построим линейную регрессию, решив задачу \ref{e:task} для выборки $ X_1 $.
    \plot{PointRegressionX1}{Точечная линейная регрессия для $ X_1 $}{p:regressionX1}
    
    Получим следующие оценки для параметров: $ \beta_0 = 0.428, \beta_1 = 2.069e^{-6} $.
    Тогда полученная модель имеет вид $ y = 0.428 + 2.069e^{-6} x $.

    Найдём для данной выборки информационное множество.
    \plot{InformSetX1}{Информационное множество для $ X_1 $}{p:informSetX1}

    На рис. \ref{p:informSetX1} можно заметит, что найденные параметры
    $ \beta_0, \beta_1 $ решением задачи \ref{e:task} лежат вне информационного множества.

    Построим коридор совместных значений для выборки $ X_1 $ и информационного множества \ref{p:informSetX1}
    и оценим значения выходной переменной $ y $ вне пределов значений входной переменной $ x $.
    \plot{InformSetCorridorX1}{Коридор совместных значений для $ X_1 $}{p:informSetCorridorX1}

    На рис. \ref{p:informSetCorridorX1} видно, что построенная точечная регрессия лежит вне коридора совместных значений,
    что согласуется с рис. \ref{p:informSetX1}.

    Проведём аналогичные построения для выборки $ X_2 $, полученной из $ X_1 $, расширением радиусов
    всех интервалов на $ 0.05 $. $ X_2 $ имеет вид.
    \plot{X2}{Вторая выборка, $ X_2 $}{p:sampleX2}
    
    Индекс Жаккра $ X_2 $ равен $ JK(X_2) = 0.4775 $.

    Построим точечную линейную регрессию для $ X_2 $.
    \plot{PointRegressionX2}{Точечная линейная регрессия для $ X_2 $}{p:regressionX2}

    Для $ X_2 $ получили следующие оценки параметров: $ \beta_0 = -0.424, \beta_1 = 3.873e^{-6} $.

    Построим информационное множество и коридор совместных значений для $ X_2 $.

    \plot{InformSetX2}{Пустое информационное множество для $ X_2 $}{p:informSetX2}
    \plot{InformSetCorridorX2}{Коридор совместных значений для $ X_2 $}{p:informSetCorridorX2}

    В итоге для $ X_2 $ получили пустое информационное множество,
    и значит для $ X_2 $ и модели \ref{e:model} не существует коридора совместных значений.

    \section{Обсуждение}
    \quad Из полученных результатов можно заметить следующее.
    Может оказаться, что, в случае малой совместности или в случае отсутствия совместности,
    точечная регрессия не попадает в информационное множество,
    что видно на рис. \ref{p:informSetX1}, \ref{p:informSetCorridorX1}, \ref{p:informSetX2}.
    Также видно, что точечная регрессия может не пересекать все интервалы исходной выборки рис. \ref{p:regressionX1}.
    Стоит отметить, что с уменьшением степени совместности, размер информационного множества и
    ширина коридора совместности уменьшаются, и в определённый момент информационное множество может оказатся пустым.
    (рис. \ref{p:informSetX1}, \ref{p:informSetCorridorX1}, \ref{p:informSetX2}, \ref{p:informSetCorridorX2}),
    что вполне ожидаемо
    При этом для обоих выборок оценки параметров, полученных с помощью точечной линейной регрессии, мало отличаются.
    Также заметно, что ширина коридора сильно увеличивается за пределами
    значений входной переменной \ref{p:informSetCorridorX1}.

\end{document}